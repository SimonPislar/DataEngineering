{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 17:45:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"Part_A_Simon_Pislar_A3\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "def to_lower_split(rdd, split_by_char):\n",
    "    return rdd.map(lambda line: line.lower().split(split_by_char))\n",
    "\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b65f20feba8f881",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process check: [['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.'], ['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.'], ['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days,', 'during', 'this', 'part-session.'], ['in', 'the', 'meantime,', 'i', 'should', 'like', 'to', 'observe', 'a', \"minute'\", 's', 'silence,', 'as', 'a', 'number', 'of', 'members', 'have', 'requested,', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned,', 'particularly', 'those', 'of', 'the', 'terrible', 'storms,', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union.'], ['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.'], ['(the', 'house', 'rose', 'and', 'observed', 'a', \"minute'\", 's', 'silence)'], ['madam', 'president,', 'on', 'a', 'point', 'of', 'order.'], ['you', 'will', 'be', 'aware', 'from', 'the', 'press', 'and', 'television', 'that', 'there', 'have', 'been', 'a', 'number', 'of', 'bomb', 'explosions', 'and', 'killings', 'in', 'sri', 'lanka.'], ['one', 'of', 'the', 'people', 'assassinated', 'very', 'recently', 'in', 'sri', 'lanka', 'was', 'mr', 'kumar', 'ponnambalam,', 'who', 'had', 'visited', 'the', 'european', 'parliament', 'just', 'a', 'few', 'months', 'ago.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the sample: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A.1.1 Read the English transcripts with Spark, and count the number of lines\n",
    "# A.2.1 Pre-process the text from both RDDs\n",
    "path_to_english_transcripts = \"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.en\"\n",
    "english_transcripts_rdd = spark_context.textFile(path_to_english_transcripts)\n",
    "pre_processed_english_text = to_lower_split(english_transcripts_rdd, ' ')\n",
    "print(f\"Pre-process check: {pre_processed_english_text.take(10)}\")\n",
    "num_lines_sample = pre_processed_english_text.count()\n",
    "print(f\"Number of lines in the sample: {num_lines_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573d83a7f89f54c2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process check: [['återupptagande', 'av', 'sessionen'], ['jag', 'förklarar', 'europaparlamentets', 'session', 'återupptagen', 'efter', 'avbrottet', 'den', '17', 'december.', 'jag', 'vill', 'på', 'nytt', 'önska', 'er', 'ett', 'gott', 'nytt', 'år', 'och', 'jag', 'hoppas', 'att', 'ni', 'haft', 'en', 'trevlig', 'semester.'], ['som', 'ni', 'kunnat', 'konstatera', 'ägde', '\"den', 'stora', 'år', '2000-buggen\"', 'aldrig', 'rum.', 'däremot', 'har', 'invånarna', 'i', 'ett', 'antal', 'av', 'våra', 'medlemsländer', 'drabbats', 'av', 'naturkatastrofer', 'som', 'verkligen', 'varit', 'förskräckliga.'], ['ni', 'har', 'begärt', 'en', 'debatt', 'i', 'ämnet', 'under', 'sammanträdesperiodens', 'kommande', 'dagar.'], ['till', 'dess', 'vill', 'jag', 'att', 'vi,', 'som', 'ett', 'antal', 'kolleger', 'begärt,', 'håller', 'en', 'tyst', 'minut', 'för', 'offren', 'för', 'bl.a.', 'stormarna', 'i', 'de', 'länder', 'i', 'europeiska', 'unionen', 'som', 'drabbats.'], ['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.'], ['(parlamentet', 'höll', 'en', 'tyst', 'minut.)'], ['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.'], ['ni', 'känner', 'till', 'från', 'media', 'att', 'det', 'skett', 'en', 'rad', 'bombexplosioner', 'och', 'mord', 'i', 'sri', 'lanka.'], ['en', 'av', 'de', 'personer', 'som', 'mycket', 'nyligen', 'mördades', 'i', 'sri', 'lanka', 'var', 'kumar', 'ponnambalam,', 'som', 'besökte', 'europaparlamentet', 'för', 'bara', 'några', 'månader', 'sedan.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the sample: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for\n",
    "# each).\n",
    "# A.2.1 Pre-process the text from both RDDs\n",
    "path_to_swedish_transcripts = \"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.sv\"\n",
    "swedish_transcripts_rdd = spark_context.textFile(path_to_swedish_transcripts)\n",
    "pre_processed_swedish_text = to_lower_split(swedish_transcripts_rdd, ' ')\n",
    "print(f\"Pre-process check: {pre_processed_swedish_text.take(10)}\")\n",
    "num_lines_sample = pre_processed_swedish_text.count()\n",
    "print(f\"Number of lines in the sample: {num_lines_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e014c6ff0199b0b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# A.1.4 Count the number of partitions.\n",
    "path_to_english_transcripts = \"hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.en\"\n",
    "english_transcripts_rdd = spark_context.textFile(path_to_english_transcripts)\n",
    "num_partitions = english_transcripts_rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c36e5965c40201a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used english words: [('the', 3498375), ('of', 1659758), ('to', 1539760), ('and', 1288401), ('in', 1085993), ('that', 797516), ('a', 773522), ('is', 758050), ('for', 534242), ('we', 522849)]\n"
     ]
    }
   ],
   "source": [
    "# A.3.1 Use Spark to compute the 10 most frequently according words in the English language\n",
    "# corpus. Repeat for the other language.\n",
    "flattened_pre_processed_english_text = pre_processed_english_text.flatMap(lambda x: x)\n",
    "english_word_tuples = flattened_pre_processed_english_text.map(lambda word: (word, 1))\n",
    "english_word_occurence = english_word_tuples.reduceByKey(lambda a, b: a + b)\n",
    "english_word_occurence_sorted = english_word_occurence.sortBy(lambda word_count: word_count[1], ascending=False)\n",
    "print(f\"Most used english words: {english_word_occurence_sorted.take(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5264474d662d30f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used swedish words: [('att', 1706293), ('och', 1344830), ('i', 1050774), ('det', 924866), ('som', 913276), ('för', 908680), ('av', 738068), ('är', 694381), ('en', 620310), ('vi', 539797)]\n"
     ]
    }
   ],
   "source": [
    "# A.3.1 Use Spark to compute the 10 most frequently according words in the English language\n",
    "# corpus. Repeat for the other language.\n",
    "flattened_pre_processed_swedish_text = pre_processed_swedish_text.flatMap(lambda x: x)\n",
    "swedish_word_tuples = flattened_pre_processed_swedish_text.map(lambda word: (word, 1))\n",
    "swedish_word_occurence = swedish_word_tuples.reduceByKey(lambda a, b: a + b)\n",
    "swedish_word_occurence_sorted = swedish_word_occurence.sortBy(lambda word_count: word_count[1], ascending=False)\n",
    "print(f\"Most used swedish words: {swedish_word_occurence_sorted.take(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f5364d5eae51d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 5) / 5]\r"
     ]
    }
   ],
   "source": [
    "# A.4.1\n",
    "zipped_english_text = pre_processed_english_text.zipWithIndex()\n",
    "zipped_swedish_text = pre_processed_swedish_text.zipWithIndex()\n",
    "swapped_key_value_english = zipped_english_text.map(lambda x: (x[1], x[0]))\n",
    "swapped_key_value_swedish = zipped_swedish_text.map(lambda x: (x[1], x[0]))\n",
    "joined_swedish_english = swapped_key_value_swedish.join(swapped_key_value_english)\n",
    "filter_structure_swedish_english = joined_swedish_english.filter(lambda x: all(x[1]))\n",
    "filter_small_words_swedish_english = filter_structure_swedish_english.filter(lambda x: len(x[1][0]) <= 5 and len(x[1][1]) <= 5)\n",
    "word_pairs = filter_small_words_swedish_english.flatMap(lambda x: zip(x[1][0], x[1][1]))\n",
    "word_pair_counts = word_pairs.map(lambda x: (x, 1)).reduceByKey(add)\n",
    "most_frequent_pairs = word_pair_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "print(f\"Most Frequent Pairs: {most_frequent_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925832cacb49c42e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f9481-f215-43d2-aa78-7d4d99438a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
